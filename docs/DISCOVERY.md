Below is a concise, opinionated blueprint you can use to steer design decisions now, plus a readyâ€‘toâ€‘circulate Technical Discovery Questionnaire at the end.



â¸»



ğŸ“Œ Recommended defaults at a glance

â€¢ Graph vs procedural: Use a hybrid: a small, typed DAG â€œspineâ€ (LangGraphâ€‘style) for stable control flow; keep procedural orchestration inside nodes for flexibility.

â€¢ Memory: Layered memory: (1) hot session memory in Redis (small, keyed â€œslotsâ€); (2) pinned facts & transactions in Postgres JSONB; (3) background vector indexing; query the vector store only on triggers (cold start, knowledge intents).

â€¢ Supervisor: Start embedded in Django (lower latency, easier coupling). Break out into a microâ€‘agent coordinator once you have â‰¥3 independent agents, shared across >1 product surface, or routing p95 > 500â€¯ms.

â€¢ Resilience: Persist a minimal DAG context snapshot per request (idempotency token + edge position + tool args). Tag â€œdegraded modeâ€ in both the JSON envelope (customerâ€‘visible) and telemetry (trace attributes).

â€¢ LLM orchestration: Keep Pydantic schemas as the hard I/O boundary; adopt LCEL pipelines where composition/reuse matters.

â€¢ Models: Dualâ€‘model: small, fast router (classification/planning) + stronger model for synthesis/grounded reasoning. Add a lightweight evaluator for guardrails.

â€¢ RAG: Hybrid retrieval (structured SQL + vector) with a single embedding backend (start with pgvector for simplicity/ACID; you can swap later).

â€¢ Function routing: Prefer explicit supervisor dispatch for auditability; allow model toolâ€‘calling at the leaf level only.

â€¢ Telemetry: Async OTLP export with batching; propagate one trace_id from the HTTP boundary through agents, LLMs, and tools (W3C trace context).

â€¢ Metrics: Start with fixed logâ€‘scale hist buckets; evaluate dynamic buckets after youâ€™ve collected a few weeks of histograms.

â€¢ Deployment: One process in dev, split containers in prod per agent class or latency domain. Use Redis Streams/Celery for async chaining; keep critical paths synchronous for UX.

â€¢ Checkpoints: Move from cache to Postgres JSONB for durability; keep Redis for hot state and crossâ€‘request speed.

â€¢ HITL: Capture explicit thumbsâ€‘up/down + rationale; store annotated exemplars for SFT/DPO later; expose confidence as qualitative tiers (Low/Med/High).

â€¢ Safety: Global guardrails agent for policy + perâ€‘tool validators for inputs/outputs. Enforce call budgets, max depth, and criticalâ€‘tool whitelists.

â€¢ Release & models: Pin model versions per release; optionally discover available models at runtime to warn/prepare on drift using the /models endpoint.  



â¸»



I. Architectural Direction



1) Agent Graph Design



Recommendation: Adopt a LangGraphâ€‘like DAG for the control plane (nodes = agent roles/tools, edges = typed outcomes), while keeping procedural logic inside nodes (Python functions/coroutines).

Why: DAGs give replayability, determinism, and state inspection. Embedding procedural code within nodes preserves speed and creativity without turning everything into a graph.

How:

â€¢ Node state is a typed Pydantic model (request, working memory slots, tool results).

â€¢ Edge conditions are pure predicates over node state (e.g., needs_more_retrieval, satisfied, escalate_hitl).

â€¢ Maintain a graph registry; version graphs and emit graph_version into telemetry.



Longâ€‘lived memory without perâ€‘request vector hits

Use a threeâ€‘tier memory:

1. Hot session memory (Redis): Small â€œslotsâ€ (identity, goals, preferences, openâ€‘loops, lastâ€‘tools) keyed by session_id. Read every request; TTL minutesâ€‘hours.

2. Pinned facts/transactions (Postgres JSONB): Durable records (orders, SLAs, profiles). Query by primary keys or narrow indexes, not vectors.

3. Vector store (pgvector/Faiss) in writeâ€‘behind: Background job embeds new artifacts and updates an â€œactive memory synopsisâ€ (50â€“200 tokens) stored in Redis. Only query vectors when:

â€¢ Cold start (no session memory),

â€¢ Knowledge intent detected by router, or

â€¢ Answer quality check fails (low coverage).



Decouple the supervisor?

â€¢ Now: Keep it in Django (simpler context sharing, transaction scoping, auth).

â€¢ Later: Extract to a microâ€‘agent coordinator when: (a) you have >1 product surface; (b) routing p95 > 500â€¯ms; (c) you need independent release cadence/scaling; (d) you require language/runtime heterogeneity.



2) Resilience & Fallback Semantics

â€¢ Checkpointing: Persist a minimal DAG context snapshot per step:

{request_id, graph_version, node_id, edge_candidates, tool_args, partial_outputs, idempotency_key, retries}.

This supports replay and exactlyâ€‘once external sideâ€‘effects (via idempotency keys).

â€¢ Degraded mode tagging:

â€¢ Application envelope: {"mode": "degraded", "reason": "...", "capabilities_disabled": [...]} (userâ€‘visible).

â€¢ Telemetry: span attrs agent.degraded=true, agent.degraded.reason, llm.circuit_state=OPEN (for correlation).

â€¢ Selfâ€‘isolation thresholds (circuit breaker): Start with:

â€¢ Open circuit if (rolling error_rate â‰¥ 30% over 2 min) AND (p95 latency â‰¥ 2Ã— SLO) OR timeâ€‘outs â‰¥ 5/min.

â€¢ Halfâ€‘open after 60 s; require success_count â‰¥ 5 before closing.

â€¢ Maintain perâ€‘model/perâ€‘tool breakers to avoid global brownouts.



â¸»



II. LLM and Cognitive Layer



3) Prompt Orchestration

â€¢ LCEL vs Pydantic + tool calling: Keep Pydantic as the contract for all tool I/O and agent intents; introduce LCEL where you need composable pipelines (retrieval â†’ routing â†’ synthesis â†’ validation). This yields reusability without losing strong typing.

â€¢ Deterministic vs generative: Use a dualâ€‘model:

â€¢ Router/classifier: small, fast model for intent, safety, and plan stubs (deterministic temperature, short tokens).

â€¢ Enterprise agent: stronger model for grounded synthesis, with toolâ€‘only or toolâ€‘preferred prompting.

â€¢ Optionally add a third â€œevaluatorâ€ (cheap) to check source coverage, policy, and format before release.

â€¢ Constraining semantic drift:

â€¢ Toolâ€‘first prompting: â€œIf sources are insufficient, return INSUFFICIENT_GROUNDING.â€

â€¢ JSONâ€‘only outputs with strict Pydantic validation + regex/JSON schema guards.

â€¢ Coverage gate: require â‰¥K unique sources or coverage_score â‰¥ threshold; otherwise trigger reâ€‘retrieve or clarify.

â€¢ Argument allowlists for local_lookup (columns, operators, max_rows).

â€¢ Selfâ€‘check step: ask the model to emit a fact list tied to source_ids (not chainâ€‘ofâ€‘thought) and validate programmatically.



4) Knowledge & RAG Strategy

â€¢ Hybrid retrieval (recommended):

â€¢ Structured: direct SQL/ORM for listings, filters, sort, pagination.

â€¢ Unstructured: vector search + BM25 rerank for docs, emails, notes.

â€¢ A router chooses path or joins results (e.g., listing + owner bio).

â€¢ One embedding backend (start): Postgres + pgvector keeps data, metadata, and transactions in one DB with ACID and backups; swap to Faiss/Weaviate only if you hit memory/throughput limits.

â€¢ Function routing: Explicit supervisor dispatch for predictability/audit; allow LLM toolâ€‘calling only at leaves to reduce orchestration chatter and simplify incident analysis.



â¸»



III. Observability, Metrics, and Tracing



5) Telemetry Strategy

â€¢ OTLP transport: Use async, batched exporters (OTel SDK) to avoid inflating tail latencies. Fall back to local UDP or file when the collector is unavailable.

â€¢ Unified trace schema:

â€¢ Generate/accept a trace_id at the HTTP boundary; propagate via W3C traceparent through Django â†’ supervisor â†’ tools â†’ DB/HTTP.

â€¢ Create custom spans for llm.call, retrieval.query, tool.invoke, graph.edge_decision.

â€¢ Minimal recommended attributes:

â€¢ agent.name, agent.version, graph.version, node, edge

â€¢ llm.model, llm.provider, llm.input_tokens, llm.output_tokens, llm.cache_hit

â€¢ retrieval.topk, retrieval.latency_ms, retrieval.corpus, retrieval.source_ids

â€¢ tool.name, tool.retry, tool.circuit_state

â€¢ mode (normal/degraded), user_tier, region

â€¢ Latency histograms: Start fixed logâ€‘scale buckets (seconds):

[0.025, 0.05, 0.1, 0.2, 0.35, 0.5, 0.75, 1, 1.5, 2, 3, 5, 8, 13]

Revisit after N weeksâ€”if bucket saturation or clustering is poor, generate dynamic buckets offline and ship them as config.

â€¢ PerformanceTracker placement:

â€¢ Global middleware for request/trace scaffolding + standard tags.

â€¢ Toolâ€‘local decorators for rich, domainâ€‘specific metrics (DB, vector, thirdâ€‘party) without bloating the global middleware.



6) Prometheus/Grafana



Baseline dashboards (by team):

â€¢ Agent overview: RPS, p50/p95/p99 latency, error rate, degradedâ€‘mode rate, circuit states.

â€¢ LLM efficiency: token in/out, cost per request, latency per 1K tokens, cache hit rate.

â€¢ Retrieval: query latency, topâ€‘k distribution, coverage score, failures/timeouts, corpus skew.

â€¢ Resilience: retries, breaker opens, fallback invocations, safe_execute rate.

â€¢ User impact: timeâ€‘toâ€‘firstâ€‘token (TTFT), response streaming duration, abandonment.



Example PromQL snippets:



# p95 latency by agent

histogram_quantile(0.95, sum(rate(agent_latency_seconds_bucket[5m])) by (le, agent))



# Degraded mode rate

sum(rate(agent_degraded_total[5m])) by (agent) / sum(rate(agent_requests_total[5m])) by (agent)



# LLM cost per request (assuming you emit llm_usd_total counter)

sum(rate(llm_usd_total[5m])) by (agent) / sum(rate(agent_requests_total[5m])) by (agent)



# Breaker open fraction

sum(rate(circuit_open_total[5m])) by (component) / sum(rate(circuit_events_total[5m])) by (component)



Alerts: Start ruleâ€‘based thresholds (clear ownership, fewer false positives). Layer anomaly detection later (OTel Collector processors or Grafana ML) for early warning.

â€¢ p95 latency > SLO for 10m.

â€¢ Error rate â‰¥ 5% for 5m (per agent and per tool).

â€¢ Degradedâ€‘mode rate â‰¥ 10% for 10m.

â€¢ Breaker is OPEN > 5m.

â€¢ Cost/run spikes (>2Ã— baseline) for 15m.



â¸»



IV. Infrastructure & Scaling



7) Deployment

â€¢ Dev: Run everything inside Django for fast iteration (hot reload, single debugger).

â€¢ Prod: Separate containers for (a) supervisor+agents, (b) vector/retrieval service, (c) async workers. Scale them independently.

â€¢ Env propagation: 12â€‘factor: all config via env; ship a single config manifest (e.g., Doppler/Vault/SSM Parameter Store) and inject at deploy. Include: OPENAI_API_KEY, OTEL_EXPORTER_OTLP_ENDPOINT, OTEL_SERVICE_NAME, OTEL_RESOURCE_ATTRIBUTES, MODEL_DEFAULTS, circuit thresholds.

â€¢ Async chaining: Use Redis Streams / Celery for offâ€‘path work (long retrieval, summarization, ETL). For userâ€‘facing steps, keep synchronous until you must break the request boundary.



8) Database & Cache

â€¢ Checkpoints: Move from Django cache to Postgres JSONB (agent_checkpoints table) for crossâ€‘instance recovery and audit; retain Redis for ephemeral stepâ€‘state and debounce.

â€¢ Embeddings metadata (explainability):

â€¢ Table documents(id, source_url, owner, created_at, pipeline_version, tags jsonb)

â€¢ Table chunks(id, document_id, char_start, char_end, checksum, summary, embedding vector, metadata jsonb)

â€¢ Table retrieval_events(id, session_id, query, topk jsonb[{chunk_id, score}], created_at)

Link answers to retrieval_events.topk and surface source_url + char_range in the UI for traceable grounding.



â¸»



V. Humanâ€‘inâ€‘theâ€‘Loop (HITL) & Ethics



9) Feedback Integration

â€¢ Signals: thumbs up/down, reason codes, inline edits, and â€œciteâ€‘missingâ€ flags.

â€¢ Ingestion: write to feedback_events (rich context: prompt hash, model, sources, latency, user tier).

â€¢ Use:

â€¢ Immediate: route lowâ€‘confidence to review queue; pin accepted edits into pinned facts.

â€¢ Nearâ€‘term: curate SFT datasets and DPO pairs (good/bad).

â€¢ Longâ€‘term: train a lightweight reward model to score drafts before release.



Confidence in UI: expose qualitative tiers derived from measurable signals (coverage, agreement between router & evaluator, tool success):

â€¢ High: â‰¥2 corroborating sources, evaluator pass.

â€¢ Medium: partial coverage or minor validator warnings.

â€¢ Low: low coverage or any critical validator fail â†’ autoâ€‘HITL.



10) Governance & Safety

â€¢ Prevent recursive escalation:

â€¢ Hard call budgets (tokens/tools), max recursion depth, allowed tool DAG, and criticalâ€‘tool allowlist.

â€¢ Require human approval for destructive actions or highâ€‘impact costs.

â€¢ Central vs perâ€‘tool: Use both:

â€¢ Global guardrails agent for policy (PII, compliance, unsafe topics).

â€¢ Perâ€‘tool validators (schemas, regex, semantic checks) before and after execution.



â¸»



VI. Development Workflow & Testing



11) Chaos & Reliability Testing

â€¢ LLM/API fault injectors in CI: deterministically simulate timeouts, 429s, 5xx, slow streams, schemaâ€‘breaking outputs, and malformed tool responses.

â€¢ Replay harness: feed recorded requests into graphs with seeded random and fixed model responses (fixtures) to validate degraded paths.

â€¢ Metrics: Aggregate safe_execute_* and guarded_llm_call_* into a single â€œresilience reportâ€ by build SHA.



12) Versioning & Rollout

â€¢ Model drift: Pin exact model IDs at release time. Monitor availability & deprecations; the /models endpoint lists available models and is your programmatic source of truth; also watch the official Deprecations and Changelog pages.  

â€¢ If you detect an ID mismatch, warn & fallback (configâ€‘controlled), but do not silently swap without an ADR.

â€¢ Blueâ€‘green / canary:

â€¢ Run graph_version=N+1 behind a header or userâ€‘bucket (1â€“5%).

â€¢ Emit graph.version in spans and metrics; compare p95, error, and degraded rates before graduating.



â¸»



ğŸ“„ Technical Discovery Questionnaire (Markdown / Notionâ€‘ready)



Purpose: Canonical audit sheet to drive Sprint 3 decisions (Observability & HITL) and the next phase of agentization.



Header

â€¢ Workstream: Agents / Observability / HITL

â€¢ Owner: â˜

â€¢ Stakeholders: â˜ Product â˜ Eng â˜ Data â˜ Security â˜ Legal

â€¢ Target release: â˜

â€¢ ADR link: â˜



â¸»



I. Architectural Direction

1. Graph shape

â€¢ Decision: â˜ Hybrid DAG (recommended) â˜ Procedural only â˜ Full DAG

â€¢ Node state schema defined? â˜ Yes (link) â˜ No

â€¢ Edge predicates enumerated & tested? â˜ Yes â˜ No

2. Memory strategy

â€¢ Slots defined (identity/goals/preferences/openâ€‘loops)? â˜ Yes (list) â˜ No

â€¢ Pinned facts in Postgres JSONB? â˜ Yes â˜ No

â€¢ Vector query triggers: â˜ Cold start â˜ Knowledge intents â˜ Low coverage reâ€‘query

3. Supervisor placement

â€¢ â˜ Embedded in Django (now) â˜ Standalone coordinator (when thresholds hit)

â€¢ Extraction criteria & SLOs documented? â˜ Yes (link) â˜ No



II. LLM & Cognitive Layer

4. Orchestration

â€¢ Hard I/O boundary with Pydantic? â˜ Yes â˜ No

â€¢ LCEL pipelines identified (where composition helps)? â˜ Yes (list) â˜ No

5. Model policy

â€¢ Routing model: â˜ chosen (id) â˜ TBD

â€¢ Synthesis model: â˜ chosen (id) â˜ TBD

â€¢ Evaluator step in place? â˜ Yes â˜ No

6. Drift constraints

â€¢ JSON schema validation enforced? â˜ Yes â˜ No

â€¢ Coverage threshold (K sources / score): â˜ value

â€¢ INSUFFICIENT_GROUNDING fallback path implemented? â˜ Yes â˜ No



III. Observability & Tracing

7. Trace model

â€¢ Single trace_id from HTTP boundary? â˜ Yes â˜ No

â€¢ Custom spans: â˜ llm.call â˜ retrieval.query â˜ tool.invoke â˜ graph.edge_decision

â€¢ Standard attrs emitted (model, tokens, coverage, mode)? â˜ Yes â˜ No

8. Metrics

â€¢ Latency hist buckets agreed (list)? â˜

â€¢ SLOs per agent/tool documented? â˜ Yes â˜ No

9. Dashboards & alerts

â€¢ Grafana dashboards live? â˜ Agent â˜ LLM â˜ Retrieval â˜ Resilience

â€¢ Alerts: â˜ p95>SLO â˜ err>5% â˜ degraded>10% â˜ breaker open â˜ cost spike



IV. Infrastructure & Scaling

10. Deployment shape



â€¢ Dev: â˜ unified

â€¢ Prod: â˜ split (agents / retrieval / workers)



11. Env & secrets



â€¢ Centralized config (Vault/SSM/Doppler)? â˜ Yes â˜ No

â€¢ OTEL/Model configs versioned? â˜ Yes â˜ No



12. Async



â€¢ Redis Streams/Celery topology documented? â˜ Yes â˜ No

â€¢ Trace context propagation verified? â˜ Yes â˜ No



V. Data, Checkpoints, and RAG

13. Checkpoints



â€¢ Postgres JSONB schema (agent_checkpoints) ready? â˜ Yes â˜ No

â€¢ Idempotency keys enforced for sideâ€‘effects? â˜ Yes â˜ No



14. Explainability



â€¢ documents/chunks/retrieval_events tables deployed? â˜ Yes â˜ No

â€¢ UI shows source URLs & char ranges? â˜ Yes â˜ No



VI. HITL, Governance, Safety

15. Feedback



â€¢ Thumbs + rationale captured? â˜ Yes â˜ No

â€¢ Review queue & pinning flow defined? â˜ Yes â˜ No

â€¢ SFT/DPO dataset pipeline scheduled? â˜ Yes â˜ No



16. Safety



â€¢ Guardrails agent policies (PII, unsafe)? â˜ Yes â˜ No

â€¢ Perâ€‘tool validators (schema/regex/semantic)? â˜ Yes â˜ No

â€¢ Budgets: â˜ token â˜ tool calls â˜ recursion depth



VII. Reliability & Rollout

17. Chaos tests



â€¢ Fault injection cases implemented (timeouts/429/5xx/schema errors)? â˜ Yes â˜ No

â€¢ Degradedâ€‘mode replay scenarios in CI? â˜ Yes â˜ No



18. Model versioning



â€¢ Model IDs pinned per release? â˜ Yes â˜ No

â€¢ /models check & deprecation monitor in place? â˜ Yes â˜ No.  



19. Blueâ€‘green / canary



â€¢ Header/bucket routing config? â˜ Yes â˜ No

â€¢ graph.version emitted + compared in Grafana? â˜ Yes â˜ No



â¸»



Implementation snippets (copy/paste)



Minimal DAG snapshot (persist per step)



{

  "request_id": "uuid",

  "graph_version": "agents@1.3.2",

  "node": "retrieval",

  "edge_candidates": ["more_retrieval","satisfied","hitl"],

  "tool_args": {"query": "..."}, 

  "partial_outputs": {"topk_ids": ["c123","c456"]},

  "idempotency_key": "uuid",

  "retries": 1,

  "timestamp": "2025-10-27T10:15:33Z"

}



Response envelope (userâ€‘visible)



{

  "mode": "degraded",

  "reason": "llm.circuit_state=OPEN",

  "capabilities_disabled": ["synthesis","long_retrieval"],

  "data": {...}

}



OTel span attributes (LLM call)



agent.name=enterprise_agent

graph.version=agents@1.3.2

llm.model=<model-id>

llm.input_tokens=812

llm.output_tokens=271

llm.cache_hit=false

retrieval.coverage_score=0.74

agent.degraded=false



Circuit breaker (pseudo)



if error_rate_2m >= 0.30 and p95_latency >= 2*SLO or timeouts_last_min >= 5:

    breaker.open(cooldown=60)





â¸»



Why the /models reference matters for drift



The Models API can be queried at runtime to enumerate available models; monitoring the Deprecations and Changelog helps you plan migrations before IDs are removed or renamed. Use it for warning/telemetry, not for hotâ€‘swapping in prod without an ADR.  



â¸»



What I did for you

â€¢ Gave concrete recommendations (defaults + thresholds) for each decision point.

â€¢ Produced a readyâ€‘toâ€‘use discovery questionnaire you can paste into Notion/Confluence or commit as DISCOVERY.md.

â€¢ Included minimal JSON/PromQL/Python snippets for immediate adoption.




